# QuickRAG environment configuration
# Copy to .env and edit â€” shell env vars take precedence.

# Ollama server URL (default: http://127.0.0.1:11434)
# OLLAMA_BASE_URL=http://127.0.0.1:11434

# --- Chat LLM provider ---
# Set to "ollama" (default) or any other value (e.g. "openai", "openwebui")
# to use an OpenAI-compatible API for chat.
# LLM_PROVIDER=ollama
# LLM_MODEL=kimi-k2.5:cloud
# LLM_BASE_URL=https://your-provider.example.com/api/v1/
# LLM_API_KEY=sk-your-api-key

# --- Embedding model ---
# Embedding model name (default: embeddinggemma:latest for Ollama).
# When using an OpenAI-compatible provider, set to that provider's embedding model.
# EMBED_MODEL=embeddinggemma:latest
